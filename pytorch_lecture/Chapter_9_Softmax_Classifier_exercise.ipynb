{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter_9_Softmax_Classifier_exercise.ipynb",
      "provenance": [],
      "mount_file_id": "1piHCeJLnLgiWP9W2T75JKaSNkmxwKp9s",
      "authorship_tag": "ABX9TyOssicOFaviyqaXJSVPqxh9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mori8/NLP-Pytorch-practice/blob/main/pytorch_lecture/Chapter_9_Softmax_Classifier_exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DS41OUIehre8"
      },
      "source": [
        "# Exercise 9-2: Build a softmax classifier for Otto Group Product\n",
        "# https://www.kaggle.com/c/otto-group-product-classification-challenge\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms, datasets\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKlCNR9E4UaM"
      },
      "source": [
        "class OttoDataset(Dataset):\n",
        "  def __init__(self):\n",
        "    xy = pd.read_csv('/content/drive/MyDrive/NLP-Pytorch/data/otto_train.csv')\n",
        "    xy = xy.drop(['id'], axis=1)\n",
        "    self.len = xy.shape[0]\n",
        "    le = LabelEncoder()\n",
        "    xy['target'] = le.fit_transform(xy['target'])\n",
        "    self.y_data = torch.tensor(xy['target'].values, dtype=torch.float32)\n",
        "    self.x_data = torch.tensor(xy.drop(['target'], axis=1).values, dtype=torch.float32)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    # return one item on the index\n",
        "    return self.x_data[index], self.y_data[index]\n",
        "  \n",
        "  def __len__(self):\n",
        "    # return the data length\n",
        "    return self.len"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_zPXAtSC7wo"
      },
      "source": [
        "dataset = OttoDataset()\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_set, test_set = torch.utils.data.random_split(dataset, [train_size, test_size])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOzLOqRFD309"
      },
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    self.l1 = nn.Linear(93, 520)\n",
        "    self.l2 = nn.Linear(520, 320)\n",
        "    self.l3 = nn.Linear(320, 240)\n",
        "    self.l4 = nn.Linear(240, 120)\n",
        "    self.l5 = nn.Linear(120, 9)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.l1(x))\n",
        "    x = F.relu(self.l2(x))\n",
        "    x = F.relu(self.l3(x))\n",
        "    x = F.relu(self.l4(x))\n",
        "    return self.l5(x)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBWcrIcRHkWD"
      },
      "source": [
        "net = Net()\n",
        "# CrossEntropyLoss의 output은 float, target은 long\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(net.parameters(), lr=0.01, momentum=0.5)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQRlvbQBIAuf"
      },
      "source": [
        "def train(epoch):\n",
        "  net.train()\n",
        "  for batch_idx, d in enumerate(train_set):\n",
        "    data, target = d\n",
        "    data, target = Variable(data), Variable(target)\n",
        "    optimizer.zero_grad()\n",
        "    output = net(data).reshape(1, 9)\n",
        "    target = target.long().reshape(1)  \n",
        "    # print(output.size(), target.size())\n",
        "    loss = criterion(output, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if batch_idx % 5000 == 0:\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "          epoch, batch_idx, len(train_set),\n",
        "          100. * batch_idx / len(train_set), loss.data.item() \n",
        "      ))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jF29LhL3JCBv"
      },
      "source": [
        "def test():\n",
        "  net.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  for data, target in test_set:\n",
        "    data, target = Variable(data, volatile=True), Variable(target)\n",
        "    output = net(data).reshape(1, 9)\n",
        "    target = target.long().reshape(1)\n",
        "    test_loss += criterion(output, target).data.item()\n",
        "    pred = torch.max(output.data, 1)[1]\n",
        "    correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "  \n",
        "  test_loss /= len(test_set.dataset)\n",
        "  print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "      test_loss, correct, len(test_set),\n",
        "      100. * correct / len(test_set)\n",
        "  ))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xasR_gkjJfch",
        "outputId": "c5158992-17c5-441d-a207-bab522dd6af1"
      },
      "source": [
        "for epoch in range(1, 5):\n",
        "  train(epoch)\n",
        "  test()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/49502 (0%)]\tLoss: 2.171056\n",
            "Train Epoch: 1 [5000/49502 (10%)]\tLoss: 0.011307\n",
            "Train Epoch: 1 [10000/49502 (20%)]\tLoss: 0.114568\n",
            "Train Epoch: 1 [15000/49502 (30%)]\tLoss: 0.008966\n",
            "Train Epoch: 1 [20000/49502 (40%)]\tLoss: 1.353779\n",
            "Train Epoch: 1 [25000/49502 (51%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [30000/49502 (61%)]\tLoss: 0.015084\n",
            "Train Epoch: 1 [35000/49502 (71%)]\tLoss: 0.081543\n",
            "Train Epoch: 1 [40000/49502 (81%)]\tLoss: 0.000005\n",
            "Train Epoch: 1 [45000/49502 (91%)]\tLoss: 0.115196\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.1341, Accuracy: 9280/12376 (75%)\n",
            "\n",
            "Train Epoch: 2 [0/49502 (0%)]\tLoss: 0.750336\n",
            "Train Epoch: 2 [5000/49502 (10%)]\tLoss: 0.001803\n",
            "Train Epoch: 2 [10000/49502 (20%)]\tLoss: 0.051739\n",
            "Train Epoch: 2 [15000/49502 (30%)]\tLoss: 0.061510\n",
            "Train Epoch: 2 [20000/49502 (40%)]\tLoss: 1.235937\n",
            "Train Epoch: 2 [25000/49502 (51%)]\tLoss: 0.000000\n",
            "Train Epoch: 2 [30000/49502 (61%)]\tLoss: 0.029719\n",
            "Train Epoch: 2 [35000/49502 (71%)]\tLoss: 0.117802\n",
            "Train Epoch: 2 [40000/49502 (81%)]\tLoss: 0.000000\n",
            "Train Epoch: 2 [45000/49502 (91%)]\tLoss: 0.100939\n",
            "\n",
            "Test set: Average loss: 0.1341, Accuracy: 9307/12376 (75%)\n",
            "\n",
            "Train Epoch: 3 [0/49502 (0%)]\tLoss: 1.262532\n",
            "Train Epoch: 3 [5000/49502 (10%)]\tLoss: 0.003015\n",
            "Train Epoch: 3 [10000/49502 (20%)]\tLoss: 0.021728\n",
            "Train Epoch: 3 [15000/49502 (30%)]\tLoss: 0.059515\n",
            "Train Epoch: 3 [20000/49502 (40%)]\tLoss: 1.182616\n",
            "Train Epoch: 3 [25000/49502 (51%)]\tLoss: 0.000000\n",
            "Train Epoch: 3 [30000/49502 (61%)]\tLoss: 0.008057\n",
            "Train Epoch: 3 [35000/49502 (71%)]\tLoss: 0.067332\n",
            "Train Epoch: 3 [40000/49502 (81%)]\tLoss: 0.000000\n",
            "Train Epoch: 3 [45000/49502 (91%)]\tLoss: 0.194783\n",
            "\n",
            "Test set: Average loss: 0.1316, Accuracy: 9393/12376 (76%)\n",
            "\n",
            "Train Epoch: 4 [0/49502 (0%)]\tLoss: 1.039616\n",
            "Train Epoch: 4 [5000/49502 (10%)]\tLoss: 0.000988\n",
            "Train Epoch: 4 [10000/49502 (20%)]\tLoss: 0.021580\n",
            "Train Epoch: 4 [15000/49502 (30%)]\tLoss: 0.017532\n",
            "Train Epoch: 4 [20000/49502 (40%)]\tLoss: 1.409657\n",
            "Train Epoch: 4 [25000/49502 (51%)]\tLoss: 0.000000\n",
            "Train Epoch: 4 [30000/49502 (61%)]\tLoss: 0.003009\n",
            "Train Epoch: 4 [35000/49502 (71%)]\tLoss: 0.001834\n",
            "Train Epoch: 4 [40000/49502 (81%)]\tLoss: 0.000000\n",
            "Train Epoch: 4 [45000/49502 (91%)]\tLoss: 0.255287\n",
            "\n",
            "Test set: Average loss: 0.1355, Accuracy: 9335/12376 (75%)\n",
            "\n"
          ]
        }
      ]
    }
  ]
}