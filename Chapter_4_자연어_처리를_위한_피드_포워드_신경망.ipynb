{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter 4: 자연어 처리를 위한 피드 포워드 신경망.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOQQBhd9brCLln7uoM5BzRX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mori8/NLP-Pytorch-practice/blob/main/Chapter_4_%EC%9E%90%EC%97%B0%EC%96%B4_%EC%B2%98%EB%A6%AC%EB%A5%BC_%EC%9C%84%ED%95%9C_%ED%94%BC%EB%93%9C_%ED%8F%AC%EC%9B%8C%EB%93%9C_%EC%8B%A0%EA%B2%BD%EB%A7%9D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAotGTKpdbr6"
      },
      "source": [
        "지난 챕터에서 가장 간단한 신경망인 퍼셉트론을 살펴봤다. 하지만 단층의 퍼셉트론으로는 데이터에서 복잡한 패턴을 학습할 수 없다는 단점이 있다. 단층 퍼셉트론의 문제점을 보여주는 대표적인 예시로는 [XOR 문제](https://wikidocs.net/24958)가 있다. \n",
        "\n",
        "이 챕터에서는 2가지 피드 포워드 신경망을 살펴볼 것이다. 다층 퍼셉트론(MLP)와 합성곱 신경망(CNN)이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enP7xe68ePBH"
      },
      "source": [
        "## 다층 퍼셉트론(MLP)\n",
        "\n",
        "MLP는 퍼셉트론을 구조적으로 확장한 신경망이다. 다층 퍼셉트론이라는 이름처럼 퍼셉트론을 여러 층으로 쌓아 만든 구조다. 3장에서 다룬 퍼셉트론은 입력으로 데이터 벡터를 받고 출력값 하나를 계산했지만, MLP는 많은 퍼셉트론이 모여 있으므로 **층의 출력**은 출력값 하나가 아닌 **벡터**다.\n",
        "\n",
        "또한 층과 층 사이에 활성함수를 통한 **비선형성**을 추가할 수 있다. 비선형성을 추가해야 깊이 있는 학습이 가능해진다. 왜일까? 한 번 3개의 층을 가진 네트워크에서 $h(x) = ax$처럼 생긴 선형 활성함수를 사용했을 때를 상상해 보자. 그 결과를 살펴보면 $h(h(h(x))) = a^3x$다. 여기서 $a^3$은 상수이므로, 은닉층을 넣은 효과를 볼 수 없다. 다층 퍼셉트론에서는 꼭 비선형 활성함수를 사용해야 하는 이유다.\n",
        "\n",
        "MLP는 은닉층을 통해 모델이 중간 표현을 학습할 수 있도록 한다. 즉 하나의 직선 혹은 (일반적으로) 초평면으로 데이터 포인트가 어느 쪽에 놓여있는지 구별할 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btrMsHDT6nBu"
      },
      "source": [
        "#### 파이토치로 MLP 구현하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsZbgQrE6qdF"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MultilayerPerceptron(nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "    \"\"\"\n",
        "    매개변수:\n",
        "      input_dim(int): 입력 벡터 크기\n",
        "      hidden_dim(int): 첫번째 Linear 층의 출력 크기\n",
        "      output_dim(int): 두번째 Linear 층의 출력 크기\n",
        "    \"\"\"\n",
        "    super(MultilayerPerceptron, self).__init__()\n",
        "    self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "    self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "  \n",
        "  def forward(self, x_in, apply_softmax=False):\n",
        "    \"\"\"MLP의 정방향 계산\n",
        "    매개변수:\n",
        "      x_in(torch.Tensor): 입력 데이터 센서\n",
        "        x_in.shape는 (batch, input_dim)입니다.\n",
        "      apply_softmax(bool): 소프트맥스 활성화 함수를 위한 플래그\n",
        "        크로스 엔트로피 손실을 사용하려면 False로 지정해야 합니다.\n",
        "    반환값:\n",
        "      결과 텐서. tensor.shape은 (batch, output_dim)입니다.\n",
        "    \"\"\"\n",
        "    intermediate = F.relu(self.fc1(x_in))\n",
        "    output = self.fc2(intermediate)\n",
        "\n",
        "    if apply_softmax:\n",
        "      output = F.softmax(output, dim=1)\n",
        "    return output"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2oXGzve8_7_",
        "outputId": "8033d624-cc73-4b84-d0b8-253b0d1ae267"
      },
      "source": [
        "batch_size = 2  # 한 번에 입력할 샘플 개수\n",
        "input_dim = 3\n",
        "hidden_dim = 100\n",
        "output_dim = 4\n",
        "\n",
        "mlp = MultilayerPerceptron(input_dim, hidden_dim, output_dim)\n",
        "print(mlp)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MultilayerPerceptron(\n",
            "  (fc1): Linear(in_features=3, out_features=100, bias=True)\n",
            "  (fc2): Linear(in_features=100, out_features=4, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtV89OLm9v7e",
        "outputId": "f3f191e3-c4a1-4f5a-ccfd-3ff09087c5d9"
      },
      "source": [
        "# 랜덤 입력으로 MLP 테스트\n",
        "def describe(x):\n",
        "  print(\"타입: {}\".format(x.type()))\n",
        "  print(\"크기: {}\".format(x.shape))\n",
        "  print(\"값: \\n{}\".format(x))\n",
        "\n",
        "x_input = torch.rand(batch_size, input_dim)\n",
        "describe(x_input)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "타입: torch.FloatTensor\n",
            "크기: torch.Size([2, 3])\n",
            "값: \n",
            "tensor([[0.8982, 0.9495, 0.5222],\n",
            "        [0.9737, 0.8029, 0.2719]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQjP0W7gJOXr",
        "outputId": "d7ccede8-d051-48ae-8a45-a2be90532c05"
      },
      "source": [
        "y_output = mlp(x_input, apply_softmax=False)\n",
        "describe(y_output)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "타입: torch.FloatTensor\n",
            "크기: torch.Size([2, 4])\n",
            "값: \n",
            "tensor([[ 0.3929, -0.2453, -0.0381,  0.1673],\n",
            "        [ 0.3477, -0.2822, -0.0290,  0.1500]], grad_fn=<AddmmBackward>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_R7H5kZJoQa"
      },
      "source": [
        "파이토치 모델의 입력과 출력을 읽는 방법을 알아보자. 위 코드에서 MLP 모델의 출력은 행 2개와 열 4개가 있는 텐서다. 이 텐서의 **행**은 **배치 차원**, 즉 **미니배치의 데이터 포인트 개수**며 **열**은 각 **데이터 포인트에 대한 최종 특성 벡터**다. 분류 등에서는 특성 벡터가 곧 예측 벡터이며, 각 값이 **확률 분포에 대응**한다.\n",
        "\n",
        "훈련을 수행하는지 추론을 수행하는지에 따라 예측 벡터로 하는 일이 다르다. 훈련 중에는 예측 벡터를 손실 함수, 타겟 클래스 레이블과 함께 사용한다. 추론에서 예측 벡터를 **확률**로 바꾸려면 `softmax` 활성함수가 필요하다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYw5_7F0L01q",
        "outputId": "fef162a6-2eaf-47c6-ddf1-47ccdffbc1b8"
      },
      "source": [
        "y_output = mlp(x_input, apply_softmax=True)\n",
        "describe(y_output)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "타입: torch.FloatTensor\n",
            "크기: torch.Size([2, 4])\n",
            "값: \n",
            "tensor([[0.3360, 0.1775, 0.2184, 0.2681],\n",
            "        [0.3290, 0.1752, 0.2258, 0.2700]], grad_fn=<SoftmaxBackward>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqAkomRwJoW3"
      },
      "source": [
        ""
      ]
    }
  ]
}